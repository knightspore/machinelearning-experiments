{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import util\n",
    "import pandas as pd\n",
    "\n",
    "import pysrt\n",
    "\n",
    "import ebooklib\n",
    "from ebooklib import epub, utils\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_embeddings(sentences): \n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "def get_epub_df(path):\n",
    "    book = epub.read_epub(path)\n",
    "    chapters = []\n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        name = item.get_name()\n",
    "        content = item.get_content().decode(\"utf-8\")\n",
    "        text = BeautifulSoup(content, \"html.parser\").get_text()\n",
    "        chapters += [text]\n",
    "    df = pd.DataFrame(chapters, columns=[\"text\"])\n",
    "    df['embedding'] = df['text'].apply(lambda x: get_embeddings(x))\n",
    "    return df\n",
    "\n",
    "def search_embeddings(term, df):\n",
    "    query = { \"search\": term, \"embedding\": get_embeddings(term) }\n",
    "    df[\"relevance\"] = df[\"embedding\"].apply(lambda x: util.cos_sim(x, query[\"embedding\"]))\n",
    "    return df.sort_values(by=[\"relevance\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index EPUBs in Direcotry (Recursively)\n",
    "\n",
    "import glob, os\n",
    "\n",
    "path = \"/run/media/c/Slem500Ext/CalibreLibrary/\"\n",
    "\n",
    "def get_book_name(path):\n",
    "    parts = path.split(\"/\")\n",
    "    return parts[len(parts)-1]\n",
    "\n",
    "dfs = pd.DataFrame(columns=[\"title\", \"text\",\"embedding\",\"path\"])\n",
    "\n",
    "for p in glob.glob(path+\"**/*/*.epub\"):\n",
    "    \n",
    "    print(f\"Found: '{get_book_name(p)}'. Getting Embeddings...\")\n",
    "    \n",
    "    book_df = get_epub_df(p)\n",
    "    \n",
    "    book_df[\"title\"] = get_book_name(p)\n",
    "    book_df[\"path\"] = p\n",
    "        \n",
    "    dfs = pd.concat([dfs,book_df], axis=0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Vector Search on the Results\n",
    "\n",
    "query = \"how do i test a function?\"\n",
    "\n",
    "df = search_embeddings(query, dfs)\n",
    "\n",
    "print(f\"Top Results for Query: '{query}'\")\n",
    "\n",
    "top10 = df.head(30).to_numpy()\n",
    "\n",
    "for item in top10:\n",
    "    print()\n",
    "    title, text = item[0], item[1]\n",
    "    print(f'Book: {title}')\n",
    "    print()\n",
    "    print(f'Chapter: \"{\" \".join(line.strip() for line in text[:80].splitlines()).strip()}...\"')\n",
    "    print()\n",
    "    print(\"--- --- ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
