{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# Discriminator Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride,\n",
    "                      bias=False, padding_mode=\"reflect\", padding=1),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True), # Changed from BatchNorm2d - use in both clocks to get better results, as the authors switched out in the CycleGAN paper\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "      return self.conv(x)\n",
    "    \n",
    "\n",
    "# Send in sat image, real image => x , y <== conact these along channels\n",
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, in_channels=3, features=[64, 128, 256, 512], out_channels=1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.initial = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels*2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t)\n",
    "  \n",
    "\t\tlayers = []\n",
    "\t\tin_channels = features[0]\n",
    "\t\tfor feature in features[1:]:\n",
    "\t\t\tlayers.append(\n",
    "\t\t\t\tCNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
    "\t\t\t)\n",
    "\t\t\tin_channels = feature\n",
    "   \n",
    "\t\tlayers.append(\n",
    "\t\t\tnn.Conv2d(\n",
    "\t\t\t\tin_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\",\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.model = nn.Sequential(*layers)\n",
    "  \n",
    "\tdef forward(self, x, y):\n",
    "\t\tx = torch.cat([x,y], dim=1)\n",
    "\t\tx = self.initial(x)\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "def test():\n",
    "  x = torch.rand((1, 3, 256, 256))\n",
    "  y = torch.rand((1, 3, 256, 256))\n",
    "  model = Discriminator()\n",
    "  preds = model(x,y)\n",
    "  print(preds.shape)\n",
    "  \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Generator\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1,\n",
    "                      bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            # Also changed from BatchNorm2d\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.down1 = Block(features, features*2, down=True,\n",
    "                           act=\"Leaky\", use_dropout=False)  # 64\n",
    "        self.down2 = Block(features*2, features*4, down=True,\n",
    "                           act=\"Leaky\", use_dropout=False)  # 32\n",
    "        self.down3 = Block(features*4, features*8, down=True,\n",
    "                           act=\"Leaky\", use_dropout=False)  # 16\n",
    "        self.down4 = Block(features*8, features*8, down=True,\n",
    "                           act=\"Leaky\", use_dropout=False)  # 8\n",
    "        self.down5 = Block(features*8, features*8, down=True,\n",
    "                           act=\"Leaky\", use_dropout=False)  # 4\n",
    "        self.down6 = Block(features*8, features*8, down=True,\n",
    "                           act=\"Leaky\", use_dropout=False)  # 2\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1,\n",
    "                      padding_mode=\"reflect\"), nn.ReLU(),  # 1x1\n",
    "        )\n",
    "        self.up1 = Block(features*8, features*8, down=False,\n",
    "                         act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(features*8*2, features*8, down=False,\n",
    "                         act=\"relu\", use_dropout=True)\n",
    "        self.up3 = Block(features*8*2, features*8, down=False,\n",
    "                         act=\"relu\", use_dropout=True)\n",
    "        self.up4 = Block(features*8*2, features*8, down=False,\n",
    "                         act=\"relu\", use_dropout=False)\n",
    "        self.up5 = Block(features*8*2, features*4, down=False,\n",
    "                         act=\"relu\", use_dropout=False)\n",
    "        self.up6 = Block(features*4*2, features*2, down=False,\n",
    "                         act=\"relu\", use_dropout=False)\n",
    "        self.up7 = Block(features*2*2, features, down=False,\n",
    "                         act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, in_channels, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This shape mirrors u-net architecture\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], dim=1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], dim=1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], dim=1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], dim=1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], dim=1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], dim=1))\n",
    "        return self.final_up(torch.cat([up7, d1], dim=1))\n",
    "\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((1, 3, 256, 256))\n",
    "    model = Generator(in_channels=3, features=64)\n",
    "    gen = model(x)\n",
    "    print(gen.shape)\n",
    "\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def save_some_examples(gen, val_loader, epoch, folder):\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "    gen.train()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "\n",
    "both_transform = A.Compose(\n",
    "    [A.Resize(width=256, height=256), ], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_only_input = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[\n",
    "                    0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    ")\n",
    "\n",
    "transform_only_target = A.Compose(\n",
    "\t[\n",
    "\t\tA.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
    "\t\tToTensorV2()\n",
    "\t]\n",
    ")\n",
    "\n",
    "class MapLoader(Dataset):\n",
    "    def __init__(self, root_dir, img_size):\n",
    "        self.img_size = img_size\n",
    "        self.root_dir = root_dir\n",
    "        self.list_files = os.listdir(self.root_dir)\n",
    "        print(self.list_files[:8])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.list_files[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        image = np.array(Image.open(img_path))\n",
    "        input_image = image[:, :self.img_size, :]\n",
    "        target_image = image[:, self.img_size:, :]\n",
    "        augmentations = both_transform(\n",
    "            image=input_image, image0=target_image)\n",
    "        input_image, target_image = augmentations[\"image\"], augmentations[\"image0\"]\n",
    "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
    "        target_image = transform_only_target(image=target_image)[\"image\"]\n",
    "        \n",
    "        return input_image, target_image\n",
    "    \n",
    "    \n",
    "class TLLLoader(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.left = os.listdir(self.root_dir + \"/left\")\n",
    "        self.right = os.listdir(self.root_dir + \"/right\")\n",
    "        print(self.left[:8])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.left)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_left = self.left[index]\n",
    "        img_right = self.right[index]\n",
    "        img_path_left = os.path.join(self.root_dir + \"/left\", img_left)\n",
    "        img_path_right = os.path.join(self.root_dir + \"/right\", img_right)\n",
    "        image_l = np.array(Image.open(img_path_left))\n",
    "        image_r = np.array(Image.open(img_path_right))\n",
    "        input_image = image_l\n",
    "        target_image = image_r\n",
    "        augmentations = both_transform(\n",
    "            image=input_image, image0=target_image)\n",
    "        input_image, target_image = augmentations[\"image\"], augmentations[\"image0\"]\n",
    "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
    "        target_image = transform_only_target(image=target_image)[\"image\"]\n",
    "        \n",
    "        return input_image, target_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.jpg', '10.jpg', '100.jpg', '1000.jpg', '1001.jpg', '1002.jpg', '1003.jpg', '1004.jpg']\n",
      "['1.jpg', '10.jpg', '100.jpg', '101.jpg', '102.jpg', '103.jpg', '104.jpg', '105.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [05:57<00:00,  7.60s/it, D_loss=0.232, G_loss=26.9, e=0/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:51<00:00,  3.65s/it, D_loss=0.671, G_loss=27.7, e=1/100] \n",
      "100%|██████████| 47/47 [03:06<00:00,  3.97s/it, D_loss=0.418, G_loss=24.5, e=2/100]\n",
      "100%|██████████| 47/47 [03:08<00:00,  4.02s/it, D_loss=0.414, G_loss=21.1, e=3/100]\n",
      "100%|██████████| 47/47 [02:47<00:00,  3.57s/it, D_loss=0.0674, G_loss=24, e=4/100]  \n",
      "100%|██████████| 47/47 [03:02<00:00,  3.88s/it, D_loss=0.443, G_loss=20.3, e=5/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:07<00:00,  4.00s/it, D_loss=0.415, G_loss=19.7, e=6/100]\n",
      "100%|██████████| 47/47 [02:59<00:00,  3.83s/it, D_loss=0.473, G_loss=20.8, e=7/100]\n",
      "100%|██████████| 47/47 [02:51<00:00,  3.64s/it, D_loss=0.278, G_loss=18.9, e=8/100]\n",
      "100%|██████████| 47/47 [02:53<00:00,  3.68s/it, D_loss=0.539, G_loss=18.8, e=9/100]\n",
      "100%|██████████| 47/47 [03:07<00:00,  3.98s/it, D_loss=0.364, G_loss=18.2, e=10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:01<00:00,  3.86s/it, D_loss=0.313, G_loss=18.9, e=11/100]\n",
      "100%|██████████| 47/47 [03:00<00:00,  3.84s/it, D_loss=0.28, G_loss=19.1, e=12/100] \n",
      "100%|██████████| 47/47 [03:00<00:00,  3.84s/it, D_loss=0.322, G_loss=20.5, e=13/100]\n",
      "100%|██████████| 47/47 [02:53<00:00,  3.69s/it, D_loss=0.55, G_loss=17.3, e=14/100] \n",
      "100%|██████████| 47/47 [02:59<00:00,  3.81s/it, D_loss=0.335, G_loss=17.5, e=15/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:03<00:00,  3.90s/it, D_loss=0.33, G_loss=17.2, e=16/100] \n",
      "100%|██████████| 47/47 [02:59<00:00,  3.83s/it, D_loss=0.253, G_loss=16, e=17/100]  \n",
      "100%|██████████| 47/47 [03:01<00:00,  3.86s/it, D_loss=0.421, G_loss=18.7, e=18/100]\n",
      "100%|██████████| 47/47 [02:50<00:00,  3.63s/it, D_loss=0.392, G_loss=14.5, e=19/100]\n",
      "100%|██████████| 47/47 [02:48<00:00,  3.58s/it, D_loss=0.524, G_loss=17.1, e=20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:53<00:00,  3.68s/it, D_loss=0.711, G_loss=14.9, e=21/100]\n",
      "100%|██████████| 47/47 [03:00<00:00,  3.83s/it, D_loss=0.294, G_loss=16, e=22/100]  \n",
      "100%|██████████| 47/47 [03:00<00:00,  3.84s/it, D_loss=0.162, G_loss=17.6, e=23/100]\n",
      "100%|██████████| 47/47 [03:02<00:00,  3.88s/it, D_loss=0.783, G_loss=15.9, e=24/100] \n",
      "100%|██████████| 47/47 [02:54<00:00,  3.72s/it, D_loss=0.266, G_loss=20.1, e=25/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:04<00:00,  3.92s/it, D_loss=0.0474, G_loss=17.2, e=26/100]\n",
      "100%|██████████| 47/47 [02:53<00:00,  3.70s/it, D_loss=0.0226, G_loss=17.4, e=27/100] \n",
      "100%|██████████| 47/47 [02:52<00:00,  3.66s/it, D_loss=0.129, G_loss=17.1, e=28/100] \n",
      "100%|██████████| 47/47 [02:50<00:00,  3.62s/it, D_loss=0.106, G_loss=15.8, e=29/100] \n",
      "100%|██████████| 47/47 [03:01<00:00,  3.85s/it, D_loss=0.226, G_loss=16.1, e=30/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:56<00:00,  3.76s/it, D_loss=0.0464, G_loss=18.2, e=31/100]\n",
      "100%|██████████| 47/47 [02:51<00:00,  3.64s/it, D_loss=0.319, G_loss=16.8, e=32/100] \n",
      "100%|██████████| 47/47 [02:57<00:00,  3.77s/it, D_loss=0.105, G_loss=15.9, e=33/100] \n",
      "100%|██████████| 47/47 [03:01<00:00,  3.85s/it, D_loss=0.00519, G_loss=18.1, e=34/100]\n",
      "100%|██████████| 47/47 [02:52<00:00,  3.67s/it, D_loss=0.0159, G_loss=19.7, e=35/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:53<00:00,  3.70s/it, D_loss=0.0804, G_loss=19.1, e=36/100] \n",
      "100%|██████████| 47/47 [02:58<00:00,  3.79s/it, D_loss=0.00854, G_loss=20.2, e=37/100]\n",
      "100%|██████████| 47/47 [03:00<00:00,  3.85s/it, D_loss=0.592, G_loss=12.4, e=38/100]  \n",
      "100%|██████████| 47/47 [02:59<00:00,  3.82s/it, D_loss=0.366, G_loss=15.1, e=39/100]\n",
      "100%|██████████| 47/47 [02:57<00:00,  3.77s/it, D_loss=0.0404, G_loss=15.7, e=40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:01<00:00,  3.85s/it, D_loss=0.0219, G_loss=15.7, e=41/100] \n",
      "100%|██████████| 47/47 [03:00<00:00,  3.84s/it, D_loss=0.00794, G_loss=17.8, e=42/100]\n",
      "100%|██████████| 47/47 [02:52<00:00,  3.67s/it, D_loss=0.00369, G_loss=17.5, e=43/100]\n",
      "100%|██████████| 47/47 [03:01<00:00,  3.87s/it, D_loss=0.0351, G_loss=16.6, e=44/100] \n",
      "100%|██████████| 47/47 [02:53<00:00,  3.68s/it, D_loss=0.00765, G_loss=19.2, e=45/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:53<00:00,  3.69s/it, D_loss=0.456, G_loss=11.5, e=46/100]  \n",
      "100%|██████████| 47/47 [02:55<00:00,  3.74s/it, D_loss=0.522, G_loss=13.7, e=47/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.73s/it, D_loss=0.563, G_loss=15, e=48/100]  \n",
      "100%|██████████| 47/47 [02:51<00:00,  3.64s/it, D_loss=0.515, G_loss=12.5, e=49/100]\n",
      "100%|██████████| 47/47 [02:58<00:00,  3.80s/it, D_loss=0.564, G_loss=11.8, e=50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:01<00:00,  3.87s/it, D_loss=0.378, G_loss=11.8, e=51/100]\n",
      "100%|██████████| 47/47 [02:50<00:00,  3.63s/it, D_loss=0.401, G_loss=13.4, e=52/100]\n",
      "100%|██████████| 47/47 [02:56<00:00,  3.77s/it, D_loss=0.432, G_loss=12.8, e=53/100]\n",
      "100%|██████████| 47/47 [02:56<00:00,  3.76s/it, D_loss=0.334, G_loss=12.9, e=54/100]\n",
      "100%|██████████| 47/47 [02:59<00:00,  3.81s/it, D_loss=0.386, G_loss=14.2, e=55/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:54<00:00,  3.72s/it, D_loss=0.405, G_loss=12.1, e=56/100]\n",
      "100%|██████████| 47/47 [02:49<00:00,  3.61s/it, D_loss=0.46, G_loss=12.4, e=57/100] \n",
      "100%|██████████| 47/47 [02:55<00:00,  3.73s/it, D_loss=0.383, G_loss=13, e=58/100]  \n",
      "100%|██████████| 47/47 [03:00<00:00,  3.84s/it, D_loss=0.398, G_loss=13.2, e=59/100]\n",
      "100%|██████████| 47/47 [03:01<00:00,  3.86s/it, D_loss=0.485, G_loss=10.9, e=60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:56<00:00,  3.75s/it, D_loss=0.356, G_loss=12.6, e=61/100]\n",
      "100%|██████████| 47/47 [02:52<00:00,  3.67s/it, D_loss=0.296, G_loss=11.4, e=62/100]\n",
      "100%|██████████| 47/47 [02:56<00:00,  3.77s/it, D_loss=0.188, G_loss=14.3, e=63/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.73s/it, D_loss=0.524, G_loss=11.6, e=64/100] \n",
      "100%|██████████| 47/47 [03:03<00:00,  3.91s/it, D_loss=0.263, G_loss=10.7, e=65/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:54<00:00,  3.70s/it, D_loss=0.481, G_loss=11.1, e=66/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.74s/it, D_loss=0.0408, G_loss=14.3, e=67/100]\n",
      "100%|██████████| 47/47 [03:09<00:00,  4.02s/it, D_loss=0.0204, G_loss=15.5, e=68/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.74s/it, D_loss=0.0152, G_loss=19.4, e=69/100]\n",
      "100%|██████████| 47/47 [02:54<00:00,  3.70s/it, D_loss=0.539, G_loss=12, e=70/100]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:50<00:00,  3.62s/it, D_loss=0.275, G_loss=14.8, e=71/100]\n",
      "100%|██████████| 47/47 [03:06<00:00,  3.96s/it, D_loss=0.587, G_loss=10.9, e=72/100]  \n",
      "100%|██████████| 47/47 [02:54<00:00,  3.72s/it, D_loss=0.296, G_loss=14, e=73/100]   \n",
      "100%|██████████| 47/47 [03:02<00:00,  3.88s/it, D_loss=0.0159, G_loss=15.3, e=74/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.73s/it, D_loss=0.0201, G_loss=15.3, e=75/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:01<00:00,  3.86s/it, D_loss=0.0104, G_loss=17.4, e=76/100]\n",
      "100%|██████████| 47/47 [03:04<00:00,  3.93s/it, D_loss=0.0215, G_loss=16.7, e=77/100] \n",
      "100%|██████████| 47/47 [02:57<00:00,  3.78s/it, D_loss=0.884, G_loss=13.6, e=78/100]  \n",
      "100%|██████████| 47/47 [02:49<00:00,  3.61s/it, D_loss=0.0774, G_loss=13.5, e=79/100]\n",
      "100%|██████████| 47/47 [02:52<00:00,  3.68s/it, D_loss=0.0115, G_loss=15.9, e=80/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:52<00:00,  3.68s/it, D_loss=0.00665, G_loss=14.6, e=81/100]\n",
      "100%|██████████| 47/47 [02:57<00:00,  3.77s/it, D_loss=0.00396, G_loss=15.8, e=82/100]\n",
      "100%|██████████| 47/47 [02:57<00:00,  3.78s/it, D_loss=0.00281, G_loss=16.9, e=83/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.73s/it, D_loss=0.00513, G_loss=19.5, e=84/100]\n",
      "100%|██████████| 47/47 [02:56<00:00,  3.76s/it, D_loss=0.25, G_loss=11.8, e=85/100]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:59<00:00,  3.81s/it, D_loss=0.0281, G_loss=13.5, e=86/100]\n",
      "100%|██████████| 47/47 [02:51<00:00,  3.65s/it, D_loss=0.0359, G_loss=14.2, e=87/100] \n",
      "100%|██████████| 47/47 [02:51<00:00,  3.64s/it, D_loss=0.935, G_loss=9.62, e=88/100]  \n",
      "100%|██████████| 47/47 [03:00<00:00,  3.84s/it, D_loss=0.00231, G_loss=14.2, e=89/100]\n",
      "100%|██████████| 47/47 [02:55<00:00,  3.74s/it, D_loss=0.0294, G_loss=15.3, e=90/100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:55<00:00,  3.74s/it, D_loss=0.00796, G_loss=14.8, e=91/100]\n",
      "100%|██████████| 47/47 [03:02<00:00,  3.87s/it, D_loss=0.0131, G_loss=15.5, e=92/100] \n",
      "100%|██████████| 47/47 [02:48<00:00,  3.59s/it, D_loss=0.0345, G_loss=16.6, e=93/100] \n",
      "100%|██████████| 47/47 [02:52<00:00,  3.66s/it, D_loss=0.594, G_loss=13.3, e=94/100]  \n",
      "100%|██████████| 47/47 [02:59<00:00,  3.81s/it, D_loss=0.658, G_loss=11.6, e=95/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [02:58<00:00,  3.79s/it, D_loss=1.05, G_loss=8.8, e=96/100]  \n",
      "100%|██████████| 47/47 [02:53<00:00,  3.70s/it, D_loss=0.379, G_loss=10.1, e=97/100]\n",
      "100%|██████████| 47/47 [02:50<00:00,  3.64s/it, D_loss=0.312, G_loss=10.9, e=98/100]\n",
      "100%|██████████| 47/47 [03:02<00:00,  3.88s/it, D_loss=0.481, G_loss=14.6, e=99/100] \n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "D_LR = 4e-4\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS_IMG = 3\n",
    "L1_LAMBDA = 100\n",
    "NUM_EPOCHS = 100\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
    "CHECKPOINT_GEN = \"gen.pth.tar\"\n",
    "\n",
    "\n",
    "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler, epoch):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_fake = gen(x)\n",
    "            D_real = disc(x, y)\n",
    "            D_fake = disc(x, y_fake.detach())\n",
    "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
    "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
    "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            D_fake = disc(x, y_fake)\n",
    "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
    "            L1 = l1(y_fake, y) * L1_LAMBDA\n",
    "            G_loss = G_fake_loss + L1\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        loop.set_postfix(e=str(epoch)+\"/\"+str(NUM_EPOCHS),\n",
    "                         D_loss=D_loss.item(), G_loss=G_loss.item())\n",
    "\n",
    "\n",
    "def main():\n",
    "    disc = Discriminator(in_channels=CHANNELS_IMG).to(DEVICE)\n",
    "    gen = Generator(in_channels=CHANNELS_IMG).to(DEVICE)\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=D_LR, betas=(0.5, 0.999))\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "    L1_LOSS = nn.L1Loss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LR)\n",
    "        load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LR)\n",
    "\n",
    "    train_dataset = MapLoader(root_dir=\"./../_datasets/cityscapes_data/train\", img_size=IMAGE_SIZE)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "    val_dataset = MapLoader(root_dir=\"./../_datasets/cityscapes_data/val\", img_size=IMAGE_SIZE)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32,\n",
    "                            shuffle=True)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(disc, gen, train_loader, opt_disc,\n",
    "                 opt_gen, L1_LOSS, BCE, g_scaler, d_scaler, epoch)\n",
    "\n",
    "        if SAVE_MODEL and epoch % 5 == 0:\n",
    "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n",
    "\n",
    "        save_some_examples(gen, val_loader, epoch, folder=\"image\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c7a21014fc0903b333c528e26b532495acabffc408f92f7990944da68b6f70a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
